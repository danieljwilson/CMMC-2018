---
title: "R Notebook"
output: html_notebook
---

## FITTING WEIBULL: GROUP LEVEL

```{r}

# Clean up if necessary
rm(list=ls())

## Maximum likelihood estimation of some data

fixations = read.csv("data/study_1/fixations.csv")

mid_rts = fixations$fix_time[fixations$fix_num>1 & fixations$rev_fix_num>1]
first_rts = fixations$fix_time[fixations$fix_num==1]

weiblnL <- function(theta, rt){
  
  shift <- theta[1]
  shape <- theta[2]
  scale <- theta[3]
  
  if (shift>=min(rt) | any(theta<.Machine$double.neg.eps) ){ # a hack to avoid NaNs (won't work with gradient descent)
    return(1000000)
  } 
  else {
    return(
      -sum(dweibull(rt-shift,scale=scale,shape=shape,log=TRUE))
    )
  }
}

# FIRST RTS
fit <- optim(c(1,1.8,mean(first_rts)),      # mean rt is a good starting point for scale
             weiblnL, rt=first_rts)

hist(first_rts, probability = TRUE, breaks=40)
lines(0:6000, dweibull((0:6000)-fit$par[1],shape=fit$par[2], scale=fit$par[3]), col=3, lwd=2)

# MID RTS
# parameters are shift, scale, shape
fit <- optim(c(1,1.8,mean(mid_rts)),      # mean rt is a good starting point for scale
             weiblnL, rt=mid_rts)

hist(mid_rts, probability = TRUE, breaks=40)
lines(0:6000, dweibull((0:6000)-fit$par[1],shape=fit$par[2], scale=fit$par[3]), col=3, lwd=2)
fit

shift_group = fit$par[1]
shape_group = fit$par[2]
scale_group = fit$par[3]


lines(0:6000, dweibull((0:6000)-shift_group,shape=shape_group, scale=scale_group), col=3, lwd=2)


```

# ISOLATE INDIVIDUAL MIDDLE FIXATIONS 

```{r}
# Import study data
# Load Experiment V1
load("/Users/djw/Documents/pCloud Synced/PhD/PROJECTS/2017_MADE/03_CODE/2017_MADE/Analysis/Data/S_M.Rdata")
v1 <- S_M

# Load Experiment V3
load("/Users/djw/Documents/pCloud Synced/PhD/PROJECTS/2017_MADE/03_CODE/2017_MADE/Analysis/Data/v3_clean.Rdata")
v3 <- v3_clean

# Look at row (starting with 2nd fix):
df1 <- v1
df2 <- v3

# check how many first fixations
first_fix_num_1 = length(df1$Trial) 
first_fix_num_2 = length(df2$trial)

# create num_fixations for df1
for (i in 1:length(df1$Trial)){
  df1$num_fixations[i] = length(df1$imageSequence[[i]])
}

# only look at trials with more than 2 fixations
df1 <- df1[df1$num_fixations > 2,]          # leaves 3820 trials
df2 <- df2[df2$num_fixations > 2,]          # leaves 7406 trials

# check how many middle fixations each subject actually has
mid_fix_num_1 = sum(df1$num_fixations) - 2*length(df1$Trial)
mid_fix_num_2 = sum(df2$num_fixations) - 2*length(df2$trial)

# define empty df
first_fix_df1 <- data.frame(matrix(ncol = 4, nrow = first_fix_num_1))
colnames(first_fix_df1) <- c("subject", "fix_dur", "base_val", "mult_val")
middle_fix_df1 <- data.frame(matrix(ncol = 4, nrow = first_fix_num_1))
colnames(middle_fix_df1) <- c("subject", "fix_dur", "base_val", "mult_val")

first_fix_df2 <- data.frame(matrix(ncol = 4, nrow = first_fix_num_2))
colnames(first_fix_df2) <- c("subject", "fix_dur", "base_val", "mult_val")
middle_fix_df2 <- data.frame(matrix(ncol = 4, nrow = first_fix_num_2))
colnames(middle_fix_df2) <- c("subject", "fix_dur", "base_val", "mult_val")

# populate dfs with middle fixations with: subject, fixation duration, base value and mult value
counter = 1

for (i in 1:length(df1$Trial)){
  for (j in 1:(df1$num_fixations[i]-2)){
    middle_fix_df1$subject[counter] = df1$subject[i]
    middle_fix_df1$fix_dur[counter] = df1[i,48+j]
    if (j%%2==1){
      middle_fix_df1$base_val[counter] = df1$secondVal[i] / df1$secondMult[i]
      middle_fix_df1$mult_val[counter] = df1$secondMult[i]
    }
    else if (j%%2==0){
      middle_fix_df1$base_val[counter] = df1$firstVal[i] / df1$firstMult[i]
      middle_fix_df1$mult_val[counter] = df1$firstMult[i]
    }
    counter = counter + 1
  }
}

counter = 1

for (i in 1:length(df2$trial)){
  for (j in 1:(df2$num_fixations[i]-2)){
    middle_fix_df2$subject[counter] = df2$subject[i]
    middle_fix_df2$fix_dur[counter] = df2[i,33+j]
    if (j%%2==1){
      middle_fix_df2$base_val[counter] = df2$second_val[i] / df2$secondMult[i]
      middle_fix_df2$mult_val[counter] = df2$second_mult[i]
    }
    else if (j%%2==0){
      middle_fix_df2$base_val[counter] = df2$first_val[i] / df2$first_mult[i]
      middle_fix_df2$mult_val[counter] = df2$first_mult[i]
    }
    counter = counter + 1
  }
}

# remove 3 outlier values above 8 seconds (max value for cleaned v2 data is below 8 seconds)
middle_fix_df1 = middle_fix_df1[middle_fix_df1$fix_dur <8, ]
max(middle_fix_df1$fix_dur)

# flip -ve vals to make positive (how to test if justified?)
middle_fix_df1$abs_base_val = abs(middle_fix_df1$base_val)
middle_fix_df2$abs_base_val = abs(middle_fix_df2$base_val)

head(middle_fix_df1)

# First fixations
counter = 1

for (i in 1:length(df2$trial)){
  
  first_fix_df2$subject[counter] = df2$subject[i]
  first_fix_df2$fix_dur[counter] = df2[i,33]
  first_fix_df2$base_val[counter] = df2$first_val[i] / df2$first_mult[i]
  first_fix_df2$mult_val[counter] = df2$first_mult[i]
  
  counter = counter + 1
}

counter = 1

for (i in 1:length(df1$Trial)){
  
  first_fix_df1$subject[counter] = df1$subject[i]
  first_fix_df1$fix_dur[counter] = df1[i,48]
  first_fix_df1$base_val[counter] = df1$firstVal[i] / df1$firstMult[i]
  first_fix_df1$mult_val[counter] = df1$firstMult[i]
  
  counter = counter + 1
}

```

## Plot
### Middle RT histograms

```{r}


hist(middle_fix_df1$fix_dur, breaks = 30,
     freq = FALSE,
     main = "Study 1: Group Middle Fixations",
     xlab = "Fixation Duration")

hist(middle_fix_df2$fix_dur, breaks = 30,
     freq = FALSE,
     main = "Study 2: Group Middle Fixations",
     xlab = "Fixation Duration")


```

### Middle Group RTs Fitted to Weibull
NOT WORKING

```{r}
# parameters are shift, scale, shape

# Study 1

# convert mid fix durations to ms
mid_fix_df1_ms = middle_fix_df1$fix_dur*1000

fit <- optim(c(1,1.8,mean(mid_fix_df1_ms)),      # mean rt is a good starting point for scale
             weiblnL, rt=mid_fix_df1_ms)

hist(middle_fix_df1$fix_dur, probability = TRUE, breaks=40)
lines(0:6000, dweibull((0:6000)-fit$par[1],shape=fit$par[2], scale=fit$par[3]), col=3, lwd=2)
fit

# Study 2

# convert mid fix durations to ms
mid_fix_df2_ms = middle_fix_df2$fix_dur*1000

fit <- optim(c(1,1.8,mean(mid_fix_df2_ms)),      # mean rt is a good starting point for scale
             weiblnL, rt=mid_fix_df2_ms)

hist(middle_fix_df2$fix_dur, probability = TRUE, breaks=40)
lines(0:6000, dweibull((0:6000)-fit$par[1],shape=fit$par[2], scale=fit$par[3]), col=3, lwd=2)
fit
```


### Create Middle Fix Matrix by Subject

```{r}

# First, how many fixations/subject
library(data.table)
dt <- data.table(middle_fix_df1)
dt[,list(count=length(fix_dur), 
         mean=mean(fix_dur),
         sd=sd(fix_dur)),
   by=subject]

# For Study 1 can see that subjects 7/3 has few middle fixations (21)
# For Study 2 can see that subjects 139/10 and 141/1 have very few middle fixations

# Remove these subjects
middle_fix_df1 = middle_fix_df1[middle_fix_df1$subject != 7,]
middle_fix_df2 = middle_fix_df2[middle_fix_df2$subject != 139,]
middle_fix_df2 = middle_fix_df2[middle_fix_df2$subject != 141,]

# Remove first fix above 7 sec
length(first_fix_df2$subject)
first_fix_df2 = first_fix_df2[first_fix_df2$fix_dur<7,]

# Fit weibull to all subjects
# Make a matrix of times

# Study 1 or 2?
df_in = middle_fix_df1

# Find the max number of fixations for an individual
dt <- data.table(df_in)
n_cols = max(dt[,list(max_fix=max(length(fix_dur))),
                by=subject]$max_fix)

# Create empty matrix 
mid_mat = matrix(nrow =length(unique(df_in$subject)), ncol = n_cols)

# Vector with subject numbers
subjects = unique(df_in$subject)

# Fill matrix with RTs
for (i in 1:length(unique(df_in$subject))){
  x = length(df_in$fix_dur[df_in$subject == subjects[i]])
  mid_mat[i,1:x] = df_in$fix_dur[df_in$subject == subjects[i]]
}

# convert to ms from s
mid_mat = mid_mat* 1000

# Study 1 or 2?
df_in = first_fix_df1

# Find the max number of fixations for an individual
dt <- data.table(df_in)
n_cols = max(dt[,list(max_fix=max(length(fix_dur))),
                by=subject]$max_fix)

# Create empty matrix 
first_mat = matrix(nrow =length(unique(df_in$subject)), ncol = n_cols)

# Vector with subject numbers
subjects = unique(df_in$subject)

# Fill matrix with RTs
for (i in 1:length(unique(df_in$subject))){
  x = length(df_in$fix_dur[df_in$subject == subjects[i]])
  first_mat[i,1:x] = df_in$fix_dur[df_in$subject == subjects[i]]
}

# convert to ms from s
first_mat = first_mat* 1000

```

# Fit Subjects to WEIBULL and Plot


### Fit Weibull to Subjects

```{r}
# init subject fits
subject_fits = NULL

# 3x3 plot
par(mfrow=c(3,3))

# MID RTS
for (i in 1:length(mid_mat[,1])) {
  rts = mid_mat[i,]
  rts = rts[!is.na(rts)]

  # parameters are shift, scale, shape
  fit <- optim(c(1,1.8,mean(rts)),      # mean rt is a good starting point for scale
               weiblnL, rt=rts)
  
  hist(rts, probability = TRUE, breaks=40,
       main = paste0("Subject ", i, " middle fixations"))
  lines(0:6000, dweibull((0:6000)-fit$par[1],shape=fit$par[2], scale=fit$par[3]), col=3, lwd=2)
  lines(0:6000, dweibull((0:6000)-shift_group,shape=shape_group, scale=scale_group), col=2, lwd=2)

  
  subject_fits[[i]] = fit
}

# 3x3 plot
par(mfrow=c(3,3))

# FIRST RTS
for (i in 1:length(first_mat[,1])) {
  rts = first_mat[i,]
  rts = rts[!is.na(rts)]

  # parameters are shift, scale, shape
  fit <- optim(c(1,1.8,mean(rts)),      # mean rt is a good starting point for scale
               weiblnL, rt=rts)
  
  hist(rts, probability = TRUE, breaks=40,
       main = paste0("Subject ", i, " first fixations"))
  lines(0:6000, dweibull((0:6000)-fit$par[1],shape=fit$par[2], scale=fit$par[3]), col=3, lwd=2)
  lines(0:6000, dweibull((0:6000)-shift_group,shape=shape_group, scale=scale_group), col=2, lwd=2)

  
  subject_fits[[i]] = fit
}

```



### FIT SUBJECT MID RT USING LBA

```{r}

library(rtdists)

# Example
x = rlba_norm(100, A=A, b=b, t0 = t0, mean_v=v, sd_v=sd_v)

curve(dlba_norm(x, A=A, b=b, t0=t0, mean_v = v, sd_v = sd_v), ylim = c(0, 4),
      xlim=c(0,3), main="Density/PDF of LBA versions", ylab="density", xlab="response time")

# Likelihood function
lba_l <- function(par, rt){
  
  A <- par[1]
  print(A)
  b <- par[2]
  print(b)
  t0 <- par[3]
  print(t0)
  v <- par[4]
  sd_v <- par[5]
  
  if (b<=A | any(theta<.Machine$double.neg.eps) ){ # a hack to avoid NaNs (won't work with gradient descent)
   return(1000000)
  } 
  else {
    return(
      -sum(log(dlba_norm(rt, A=A, b=b, t0=t0, mean_v = v, sd_v = sd_v)))
    )
  }
}

remove(x)


curve(dlba_norm(x, A=A, b=b, t0=t0, mean_v = 1.0, sd_v = 0.5), main="Density/PDF of LBA versions", ylab="density", xlab="response time")  
# parameters are:
A <- 0.2    # start point interval or evidence in accumulator before beginning of decision process. Start point varies from trial to trial in the interval [0, A] (uniform distribution). Average amount of evidence before evidence accumulation across trials is A/2.
b <- 0.5    # response threshold. (b - A/2) is a measure of "response caution".
t0 <- 0.3   # non-decision time or response time constant (in seconds). Lower bound for the duration of all non-decisional processes (encoding and response execution).
v= 1.0      # mean and standard deviation of normal distribution for drift rate (norm). See Normal
sd_v = 0.5

# convert rts to seconds
rt_s = mid_rts/1000
parameters = c(A, b, t0, v, sd_v)

fit <- optim(par = c(A,b,t0,v,sd_v),      
             lba_l, rt=rt_s)


x = rlba_norm(100, A=A, b=b, t0=t0, mean_v=v, sd_v=sd_v)

# PLOT
hist(rt_s, probability = TRUE, breaks=40)

x = rlba_norm(1000, A=fit$par[1], b=fit$par[2], t0=fit$par[3], mean_v = fit$par[4], sd_v = fit$par[5])
curve(dlba_norm(x, A=fit$par[1], b=fit$par[2], t0=fit$par[3], mean_v = fit$par[4], sd_v = fit$par[5]),
      main="Density/PDF of LBA versions", ylab="density", xlab="response time")
remove(x)


### plot predictions

#par(mfrow=c(1,2), cex=1.2)
plot(p1_q, c(0.1, 0.3, 0.5, 0.7, 0.9), 
     pch = 2, 
     ylim=c(0, 1), xlim = c(0.3, 1.7), 
     ylab = "Cumulative Probability", 
     xlab = "Response Time (sec)", 
     main = "LBA")

points(p1_q_e, q*prop.table(table(p1$corr))[1], pch = 2)
lines(pred_correct_lba, q*pred_prop_correct_lba, type = "b")
lines(pred_error_lba, q*(1-pred_prop_correct_lba), type = "b")

legend("right", legend = c("data", "predictions"), pch = c(2, 1), lty = c(0, 1))


```



# Bin Values

```{r}
# Bin by count


```



# Weibull at each value - interpret the parameter changes

```{r}


```


### ALT LBA

```{r}

library(rtdists)


LBAnL <- function(theta, rt){
  
  # Start point varies from trial to trial in the interval [0, A] (uniform distribution).
  # Average amount of evidence before evidence accumulation across trials is A/2.
  
  A <- theta[1] 
  
  # response threshold. (b - A/2) is a measure of "response caution"
  b <- theta[2] 
  
  # non-decision time or response time constant (in seconds). Lower bound for the
  # duration of all non-decisional processes (encoding and response execution).
  t0 <- theta[3]
  
  # variability of non-decision time, such that t0 is uniformly distributed between
  # t0 and t0 + st0. Default is 0. Can be trialwise, and will be recycled to length
  # of rt.
  st0 <- theta[4]
  
  # character specifying the distribution of the drift rate.
  # Possible values are c("norm", "gamma", "frechet", "lnorm"), default is "norm".
  distribution<- theta[5]
  
  # 
    shape <- theta[2]
  scale <- theta[3]
  
  if (shift>=min(rt) | any(theta<.Machine$double.neg.eps) ){ # a hack to avoid NaNs (won't work with gradient descent)
    return(1000000)
  } 
  else {
    return(
      -sum(dweibull(rt-shift,scale=scale,shape=shape,log=TRUE))
    )
  }
}

# parameters are shift, scale, shape
fit <- optim(c(1,1.8,mean(mid_rts)),      # mean rt is a good starting point for scale
             weiblnL, rt=mid_rts)

hist(mid_rts, probability = TRUE, breaks=40)
lines(0:6000, dweibull((0:6000)-fit$par[1],shape=fit$par[2], scale=fit$par[3]), col=3, lwd=2)
fit

```


### PLOT: SECOND FIXATION TIME VS. VALUE & MULTIPLIER
```{r}
df <- v3

ggplot() +
  geom_smooth(aes(x=secondVal/secondMult, y=`2_fixation`, group = factor(secondMult), colour = factor(secondMult)), df) +
  #geom_smooth(aes(x=summedVal, y=logRT, colour = "flip"), subset(total_M_clean3, flip==1)) +
  coord_cartesian(xlim = c(-1, 1))  +
  #ggtitle("Second Fixation Timing vs Total Value")
  #geom_point(shape=1) +    # Use hollow circles
  geom_smooth() + # Add a loess smoothed fit curve with confidence region
  theme_minimal()+
  guides(colour=guide_legend("Attribute\nWeight")) +
  scale_x_continuous(name="Attribute Base Value ($)", seq(-1,1,0.2), limits = c(-1,1))+
  scale_y_continuous(name = "Attribute Dwell Time (s)") +
  theme(axis.title.x=element_text(size=14),
        axis.title.y = element_text(size = 14),
        legend.title = element_text(size = 10))+
  theme(legend.position="right")

#Test for SIG
summary(lm(`2_fixation`~secondVal + secondMult, df))
```




# EZ-DDM at each value - intepret parameter changes

```{r}


```

## Make a vector with middle fixations for each subject
## Make a vector with first fixations for each subject


