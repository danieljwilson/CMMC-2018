sims = 8
replicate(sims, size =1,prob=prob)
rbinom(sims, size =1,prob=prob)
replicate(1000, rbinom(sims, size =1,prob=prob))
x = replicate(1000, rbinom(sims, size =1,prob=prob))
dim(x)
x <- 1:12 ; dim(x)
x <- 1:12
dim(x)
prob = 0.7      # prob of heads
n = 1000
sims = 8
x = replicate(1000, rbinom(sims, size =1,prob=prob))
dim(x)
colSums(x)
# Plot hist
hist(colSums())
sums = colSums(x)
# Plot hist
hist(colSums)
# Plot hist
hist(x = colSums)
typeof(sums)
x[1]
x[2]
x[3]
x[4]
x[10]
sums[1]
sums[2]
sums[20]
sums[24]
# Plot hist
hist(colSums, freq=FALSE)
# Plot hist
hist(sums, freq=FALSE)
prob = 0.7      # prob of heads
n = 1000
sims = 8
x = replicate(1000, rbinom(sims, size=1, prob=prob))
sums = colSums(x)
# Plot hist
hist(sums, freq=FALSE)
prob = 0.7      # prob of heads
n = 1000
sims = 8
x = replicate(1000, rbinom(sims, size=1, prob=prob))
sums = colSums(x)
# Plot hist
hist(sums, freq=FALSE)
prob = 0.7      # prob of heads
n = 1000
sims = 8
x = replicate(1000, rbinom(sims, size=1, prob=prob))
sums = colSums(x)
# Plot hist
hist(sums, freq=FALSE)
prob = 0.7      # prob of heads
n = 1000
sims = 8
x = replicate(1000, rbinom(sims, size=1, prob=prob))
sums = colSums(x)
# Plot hist
hist(sums, freq=FALSE)
x = replicate(1000, rbinom(sims, size=1, prob=prob))
sums = colSums(x)
# Plot hist
hist(sums, freq=FALSE)
x = replicate(1000, rbinom(sims, size=1, prob=prob))
sums = colSums(x)
# Plot hist
hist(sums, freq=FALSE)
?dbinom
dbinom(x, 10, prob = 0.5)
x = c(1:10)
dbinom(x, 10, prob = 0.5)
hist(prob)
x = c(1:10)
prob = dbinom(x, 10, prob = 0.5)
hist(prob)
hist(x,prob)
nTosses <- 10
k <- 0:nTosses
pHeads <- 0.5
pK <- dbinom(k, nTosses, pHeads)
plot(k, pK, type="h",
xlab="Number of heads",
ylab="Probability")
# Plot hist
hist(sums,
xlab="number of heads",
freq=FALSE,
breaks=(c(0:9)-0.5))
nTrials <- 8
k <- 0:nTosses
pHeads <- 0.7
pK <- dbinom(k, nTosses, pHeads)
pK[5]
nTrials <- 8
k <- 0:nTosses
pHeads <- 0.7
pK <- dbinom(k, nTosses, pHeads)
pK[5]
nTrials <- 8
k <- 0:nTrials
pHeads <- 0.7
pK <- dbinom(k, nTrials, pHeads)
pK[5]
nTrials <- 8
k <- 0:nTrials
pHeads <- 0.7
pK <- dbinom(k, nTrials, pHeads)
pK[5]
pData <- dbinom(5, 8, 0.7)
pData
pK[6]
exp(1)
#Now we have NUMBER CORRECT FROM 20 TRIALS
observations=c(8, 7, 6, 6, 8, 10, 10, 14, 14, 19)
ntrials<-20
#probability of observing 8 recalls in 20 trials if
#predicted recall probability is 0.5
dbinom(8,ntrials,.5)
#We can do this with a whole vector of recall frequencies
#and a matching vector of predicted recall probs
dbinom(observations,ntrials,discrim)
#probability of observing 8 recalls in 20 trials if
#predicted recall probability is 0.5
dbinom(8,ntrials,.5)
#We can do this with a whole vector of recall frequencies
#and a matching vector of predicted recall probs
dbinom(observations,ntrials,discrim)
#We can then log the probabilites, then sum them
#then take the negative
-sum(log(dbinom(observations,ntrials,discrim)))
#Now write a function to calculate LL
LL = function (c,N,distances,observations) {
predictions=serpos(c, distances) #Using the function we made before
LL_est=-sum(log(dbinom(observations,N,predictions)))
LL_est=return(LL_est)
}
#We can then log the probabilites, then sum them
#then take the negative
-sum(log(dbinom(observations,ntrials,discrim)))
#Now we have NUMBER CORRECT FROM 20 TRIALS
observations=c(8, 7, 6, 6, 8, 10, 10, 14, 14, 19)
ntrials<-20
#probability of observing 8 recalls in 20 trials if
#predicted recall probability is 0.5
dbinom(8,ntrials,.5)
#We can do this with a whole vector of recall frequencies
#and a matching vector of predicted recall probs
dbinom(observations,ntrials,discrim)
#We can then log the probabilites, then sum them
#then take the negative
-sum(log(dbinom(observations,ntrials,discrim)))
#now test it
LL(5,ntrials,temp_dists,observations)
rm(list=ls())
dev.off()
c=6 #the similarity-distance parameter
retention_interval=1 #the retention interval
#First make a vector with the temporal distances of each item
#at the time of retrieval
temp_dists=c(10:1)
#Then add the retention interval
temp_dists=temp_dists+retention_interval
#Then log-transform the result
temp_dists=log(temp_dists)
#NOTE we could better have done all this in one line like this:
#temp_dists=log(c(10:1)+retention_interval)
#Now a loop that calculates discriminability
discrim=NULL                                # initalise a matrix that we will save results into
for (i in 1:length(temp_dists)){
dist = abs(temp_dists[i]-temp_dists)        # find the summed distance from all
eta = exp(-c* dist)                         # compute  summed similarities
discrim[i] = 1/sum(eta)                     # discriminability =inverse of summed similarities
}
#Now we have NUMBER CORRECT FROM 20 TRIALS
observations=c(8, 7, 6, 6, 8, 10, 10, 14, 14, 19)
ntrials<-20
#probability of observing 8 recalls in 20 trials if
#predicted recall probability is 0.5
dbinom(8,ntrials,.5)
#We can do this with a whole vector of recall frequencies
#and a matching vector of predicted recall probs
dbinom(observations,ntrials,discrim)
#We can then log the probabilites, then sum them
#then take the negative
-sum(log(dbinom(observations,ntrials,discrim)))
#Now write a function to calculate LL
LL = function (c,N,distances,observations) {
predictions=serpos(c, distances) #Using the function we made before
LL_est=-sum(log(dbinom(observations,N,predictions)))
LL_est=return(LL_est)
}
#now test it
LL(5,ntrials,temp_dists,observations)
serpos = function (c,distances){
for (i in 1:length(distances)){
eta=exp(-c*abs(distances[i]-distances))  #summed sims
discrim[i]=1/sum(eta) #discriminability
}
#now test it
LL(5,ntrials,temp_dists,observations)
#now test it
LL(5,ntrials,temp_dists,observations)
#We can then log the probabilites, then sum them
#then take the negative
-sum(log(dbinom(observations,ntrials,discrim)))
#Now write a function to calculate LL
LL = function (c,N,distances,observations) {
predictions=serpos(c, distances) #Using the function we made before
LL_est=-sum(log(dbinom(observations,N,predictions)))
LL_est=return(LL_est)
}
#now test it
LL(5,ntrials,temp_dists,observations)
#Now we have NUMBER CORRECT FROM 20 TRIALS
observations=c(8, 7, 6, 6, 8, 10, 10, 14, 14, 19)
ntrials<-20
#probability of observing 8 recalls in 20 trials if
#predicted recall probability is 0.5
dbinom(8,ntrials,.5)
#We can do this with a whole vector of recall frequencies
#and a matching vector of predicted recall probs
dbinom(observations,ntrials,discrim)
#We can then log the probabilites, then sum them
#then take the negative
-sum(log(dbinom(observations,ntrials,discrim)))
#First plot the data (observations)
plot(observations, type="p",xlim=(c(0, 10)),ylim=((c(0,1))),ylab="Prob Correct",xlab="Serial Position",las=1)
#Now we need to calculate predictions with optim's estimated parameters
best_predictions <- serpos(param_ests, temp_dists)
#then we can add the points to the graph
points(best_predictions, type="l", col="green", las=1)
ex
esc
exit
;
exit()
=
#now test it
LL(5,ntrials,temp_dists,observations)
#Now we want to create a function that will calculate a serial position curve for
#any value of c and and temporal distance vector
serpos = function (c,distances){
for (i in 1:length(distances)){
eta=exp(-c*abs(distances[i]-distances))  #summed sims
discrim[i]=1/sum(eta) #discriminability
}
output= discrim
}
#We can then log the probabilites, then sum them
#then take the negative
-sum(log(dbinom(observations,ntrials,discrim)))
#Now write a function to calculate LL
LL = function (c,N,distances,observations) {
predictions=serpos(c, distances) #Using the function we made before
LL_est=-sum(log(dbinom(observations,N,predictions)))
LL_est=return(LL_est)
}
#now test it
LL(5,ntrials,temp_dists,observations)
#Now we need to use optimize to find maximum likelihood params
results<-optimize(LL,c(0,100),N=ntrials,distances=temp_dists,observations=observations)
#The outcome is again stored in "results"
param_ests<-results$minimum #param_ests has the best-fit parame estimates
error<-results$objective    #smallest error that could be obtained
param_ests
error
#Now add the curve onto the graph we already have:
#Now we need to calculate predictions with optim's estimated parameters
best_predictions <- serpos(param_ests, temp_dists)
#then we can add the points to the graph
points(best_predictions, type="p", col="red", las=1)
#First plot the data (observations)
plot(observations, type="p",xlim=(c(0, 10)),ylim=((c(0,1))),ylab="Prob Correct",xlab="Serial Position",las=1)
#then we can add the points to the graph
points(best_predictions, type="p", col="red", las=1)
#First plot the data (observations)
plot(observations, type="p",xlim=(c(0, 10)),ylim=((c(0,1))),ylab="Prob Correct",xlab="Serial Position",las=1)
#then we can add the points to the graph
points(best_predictions, type="p", col="red", las=1)
?dweibull
x = dweibull(1000, shape, scale)
# Plot porb density for Weibull
scale = 200
shape = 2
shift = 0
x = dweibull(1000, shape, scale)
x
x = dweibull(c(1:1000), shape, scale)
plot(x)
y = dweibull(c(1:200), shape, scale)
plot(x)
plot(y)
x = dweibull(c(1:1000), shape, scale)
y = dweibull(c(1:200), shape, scale)
plot(x)
plot(y)
# What is p(data|parameters) for data = 200ms?
x[200]
plot(x)
x <- seq(0,1000,length.out = 100)
plot(x, dweibull(x,shape = shape, scale=scale),
type="l",
xlab="RT (ms)",
ylab="Prob Density")
# What is p(data|parameters) for data = 200ms?
x[200]
# Plot porb density for Weibull
scale = 200
shape = 2
shift = 0
x <- seq(0,1000,length.out = 100)
y <-  dweibull(x,shape = shape, scale=scale)
plot(x,y,
type="l",
xlab="RT (ms)",
ylab="Prob Density")
# What is p(data|parameters) for data = 200ms?
# What is p(data|parameters) for data = 200ms?
y[200]
# What is p(data|parameters) for data = 200ms?
y
# What is p(data|parameters) for data = 200ms?
y[20]
dweibull(200, shape, scale)
# What is p(data|parameters) for data = 200ms?
y[20]
dweibull(200, shape, scale)
rts = scan('rt.txt')
# Calculate log likelihood for a single data point under the Weibull
?dweibull
scale = 200
shape = 2
shift = 0
# Calculate log likelihood for a single data point under the Weibull
dweibull(rts-shift, scale=scale, shape=shape , log=TRUE)
# Eztend this calculation for multiple data points
-1*sum(dweibull(rts-shift, scale=scale, shape=shape , log=TRUE))
shape = 3
shift = 0
# Eztend this calculation for multiple data points
-1*sum(dweibull(rts-shift, scale=scale, shape=shape , log=TRUE))
shape = 2
# Eztend this calculation for multiple data points
-1*sum(dweibull(rts-shift, scale=scale, shape=shape , log=TRUE))
xout <- optim(theta, LL, rts=rts)
theta = c(scale, shape, shift)
# Wrap in a function that takes two arguments: theta (vector of parameters) and data vector (rts)
negLL = function(rts, theta){
scale = theta[1]
shape = theta[2]
shift = theta[3]
LL = -1*sum(dweibull(rts-shift, scale=scale, shape=shape , log=TRUE))
return(LL)
}
xout <- optim(theta, LL, rts=rts)
xout <- optim(theta, negLL, rts=rts)
xout
xout <- optim(theta, negLL, rts=rts)
xout
# Wrap in a function that takes two arguments: theta (vector of parameters) and data vector (rts)
negLL = function(rts, theta){
scale = theta[1]
shape = theta[2]
shift = theta[3]
LL = -1*sum(dweibull(rts-shift, scale=scale, shape=shape , log=TRUE))
if(LL == NaN){
print(theta)
}
return(LL)
}
xout <- optim(theta, negLL, rts=rts)
# Wrap in a function that takes two arguments: theta (vector of parameters) and data vector (rts)
negLL = function(rts, theta){
scale = theta[1]
shape = theta[2]
shift = theta[3]
LL = -1*sum(dweibull(rts-shift, scale=scale, shape=shape , log=TRUE))
if(LL is NaN){
# Wrap in a function that takes two arguments: theta (vector of parameters) and data vector (rts)
negLL = function(rts, theta){
scale = theta[1]
shape = theta[2]
shift = theta[3]
LL = -1*sum(dweibull(rts-shift, scale=scale, shape=shape , log=TRUE))
if(LL >2000){
print(theta)
}
return(LL)
}
xout <- optim(theta, negLL, rts=rts)
# Wrap in a function that takes two arguments: theta (vector of parameters) and data vector (rts)
negLL = function(rts, theta){
scale = theta[1]
shape = theta[2]
shift = theta[3]
LL = -1*sum(dweibull(rts-shift, scale=scale, shape=shape , log=TRUE))
return(LL)
}
xout <- optim(theta, negLL, rts=rts)
xout
# Wrap in a function that takes two arguments: theta (vector of parameters) and data vector (rts)
negLL = function(rts, theta){
scale = theta[1]
shape = theta[2]
shift = theta[3]
LL = -1*sum(dweibull(rts-shift, scale=scale, shape=shape , log=TRUE))
print(LL)
return(LL)
}
xout <- optim(theta, negLL, rts=rts)
# Wrap in a function that takes two arguments: theta (vector of parameters) and data vector (rts)
negLL = function(rts, theta){
scale = theta[1]
shape = theta[2]
shift = theta[3]
LL = -1*sum(dweibull(rts-shift, scale=scale, shape=shape , log=TRUE))
if(LL == Inf){
print(LL)
}
return(LL)
}
xout <- optim(theta, negLL, rts=rts)
# Wrap in a function that takes two arguments: theta (vector of parameters) and data vector (rts)
negLL = function(rts, theta){
scale = theta[1]
shape = theta[2]
shift = theta[3]
LL = -1*sum(dweibull(rts-shift, scale=scale, shape=shape , log=TRUE))
if(NA){
print(LL)
}
return(LL)
}
xout <- optim(theta, negLL, rts=rts)
# Wrap in a function that takes two arguments: theta (vector of parameters) and data vector (rts)
negLL = function(rts, theta){
scale = theta[1]
shape = theta[2]
shift = theta[3]
LL = -1*sum(dweibull(rts-shift, scale=scale, shape=shape , log=TRUE))
if(is.na(LL)){
print(LL)
}
return(LL)
}
xout <- optim(theta, negLL, rts=rts)
xout
xout <- optim(theta, negLL, rts=rts)
# Wrap in a function that takes two arguments: theta (vector of parameters) and data vector (rts)
negLL = function(rts, theta){
scale = theta[1]
shape = theta[2]
shift = theta[3]
LL = -1*sum(dweibull(rts-shift, scale=scale, shape=shape , log=TRUE))
if(is.na(LL)){
print(theta)
}
return(LL)
}
xout <- optim(theta, negLL, rts=rts)
xout <- optim(theta, negLL, rts=rts)
xout
## Maximum likelihood estimation of some data
rt <- scan("rt.txt")
weiblnL <- function(theta, rt){
shift <- theta[1]
shape <- theta[2]
scale <- theta[3]
if (shift>=min(rt) | any(theta<.Machine$double.neg.eps) ){ # a hack to avoid NaNs (won't work with gradient descent)
return(1000000)
}
else {
return(
-sum(dweibull(rt-shift,scale=scale,shape=shape,log=TRUE))
)
}
}
# parameters are shift, scale, shape
fit <- optim(c(1,1.8,mean(rt)),
weiblnL, rt=rt)
hist(rt, probability = TRUE, breaks=20)
lines(0:1000, dweibull((0:1000)-fit$par[1],shape=fit$par[2], scale=fit$par[3]), col=3, lwd=2)
hist(rt, probability = TRUE, breaks=20)
lines(0:1000, dweibull((0:1000)-fit$par[1],shape=fit$par[2], scale=fit$par[3]), col=3, lwd=2)
## Maximum likelihood estimation of some data
rt <- scan("rt.txt")
weiblnL <- function(theta, rt){
shift <- theta[1]
shape <- theta[2]
scale <- theta[3]
if (shift>=min(rt) | any(theta<.Machine$double.neg.eps) ){ # a hack to avoid NaNs (won't work with gradient descent)
return(1000000)
}
else {
return(
-sum(dweibull(rt-shift,scale=scale,shape=shape,log=TRUE))
)
}
}
# parameters are shift, scale, shape
fit <- optim(c(1,1.8,mean(rt)),      # mean rt is a good starting point for scale
weiblnL, rt=rt)
hist(rt, probability = TRUE, breaks=20)
lines(0:1000, dweibull((0:1000)-fit$par[1],shape=fit$par[2], scale=fit$par[3]), col=3, lwd=2)
hist(rt, probability = TRUE, breaks=20)
lines(0:1000, dweibull((0:1000)-fit$par[1],shape=fit$par[2], scale=fit$par[3]), col=3, lwd=2)
?dweibull
